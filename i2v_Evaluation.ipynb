{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3JAT+dOyn1bQt196POphO"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jc8oiquvUy4Z"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import os\n",
        "from scipy import sparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = \"ml-20m\"\n",
        "#data=\"netflix-prize\"\n",
        "#data=\"msd\"\n",
        "dir = \"/home/onishi/recommend/\" + data + \"/pro_sg/\"\n",
        "model_dir=data+\"_model\"\n",
        "model = Word2Vec.load( \"/home/onishi/recommend/\"+model_dir + \"/model_besst_300\")\n",
        "df = pd.read_csv(dir + \"train.csv\")\n",
        "size = len(df['sid'].unique())\n",
        "\n",
        "#get item-embeddings\n",
        "for i in range(size):\n",
        "    X[:,i]=model.wv[str(i)]\n",
        "\n",
        "#create I-I similarity\n",
        "similarity_matrix = cosine_similarity(X.T)\n",
        "np.fill_diagonal(similarity_matrix, 0)\n",
        "df['sid']=df['sid'].astype(str)\n",
        "\n",
        "class FIT:\n",
        "    def __init__(self):\n",
        "        self.user_enc = LabelEncoder()\n",
        "        self.item_enc = LabelEncoder()\n",
        "    def _get_users_and_items(self, df):\n",
        "        users = self.user_enc.fit_transform(df.loc[:, 'uid'])\n",
        "        items = self.item_enc.fit_transform(df.loc[:, 'sid'])\n",
        "        return users, items\n",
        "    def fit(self, df, implicit=True):\n",
        "        users, items = self._get_users_and_items(df)\n",
        "        values = np.ones(df.shape[0]) if implicit else df['rating'].to_numpy() / df['rating'].max()\n",
        "        X = csr_matrix((values, (users, items)))\n",
        "        self.X = X\n",
        "df_test=pd.read_csv(dir+\"validation_tr.csv\")\n",
        "df=pd.read_csv(dir+\"train.csv\")\n",
        "model=FIT()\n",
        "model.fit(df)\n",
        "users = df_test.loc[:, 'uid']\n",
        "items = df_test.loc[:, 'sid']\n",
        "u_enc = LabelEncoder()\n",
        "users_id = u_enc.fit_transform(users)\n",
        "items_id = model.item_enc.transform(items)\n",
        "values = np.ones(df_test.shape[0])\n",
        "shape = (u_enc.classes_.size, model.item_enc.classes_.size)\n",
        "X = csr_matrix((values, (users_id, items_id)), shape=shape)\n",
        "print(similarity_matrix.shape)\n",
        "pred = X.dot(similarity_matrix)\n",
        "def NDCG(x_pred, x_test, k=100):\n",
        "  \n",
        "    user_num = x_pred.shape[0]\n",
        "\n",
        "    idx_topk_part = bn.argpartition(-x_pred, k, axis=1)\n",
        "\n",
        "    topk_part = x_pred[np.arange(user_num)[:, np.newaxis],\n",
        "                       idx_topk_part[:, :k]]\n",
        "\n",
        "    idx_part = np.argsort(-topk_part, axis=1)\n",
        "\n",
        "    idx_topk = idx_topk_part[np.arange(user_num)[:, np.newaxis], idx_part]\n",
        "\n",
        "\n",
        "    tp = 1. / np.log2(np.arange(2, k + 2))                       \n",
        "    DCG = (x_test[np.arange(user_num)[:, np.newaxis],\n",
        "                         idx_topk].toarray() * tp).sum(axis=1)\n",
        "    IDCG = np.array([(tp[:min(n, k)]).sum()\n",
        "                     for n in x_test.getnnz(axis=1)])\n",
        "    return DCG / IDCG\n",
        "\n",
        "def Recall(x_pred, x_test, k):\n",
        "    users_num = x_pred.shape[0]\n",
        "\n",
        "    idx = bn.argpartition(-x_pred, k, axis=1)\n",
        "   \n",
        "    X_pred_binary = np.zeros_like(x_pred, dtype=bool)\n",
        "   \n",
        "    X_pred_binary[np.arange(users_num)[:, np.newaxis], idx[:, :k]] = True\n",
        "   \n",
        "    X_true_binary = (x_test > 0).toarray()\n",
        "   \n",
        "    tmp = (np.logical_and(X_true_binary, X_pred_binary).sum(axis=1)).astype(\n",
        "        np.float32)\n",
        "    recall = tmp / np.minimum(k, X_true_binary.sum(axis=1))\n",
        "    return recall\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "unique_sid = list()\n",
        "with open(os.path.join(dir, 'unique_sid.txt'), 'r') as f:\n",
        "    for line in f:\n",
        "        unique_sid.append(line.strip())\n",
        "n_items = len(unique_sid)\n",
        "validation_data_tr, validation_data_te = load_tr_te_data(\n",
        "        os.path.join(dir, 'validation_tr.csv'),\n",
        "        os.path.join(dir, 'validation_te.csv'))\n",
        "batch_size_test=2000\n",
        "N_test=validation_data_tr.shape[0]\n",
        "idx_list_test=range(N_test)\n",
        "test=validation_data_te[idx_list_test]\n",
        "n100_list,r20_list,r50_list=[],[],[]\n",
        "for bnum,st_idx in enumerate(range(0,N_test,batch_size_test)):\n",
        "    end_idx=min(st_idx+batch_size_test,N_test)\n",
        "    X = validation_data_tr[idx_list_test[st_idx:end_idx]]\n",
        "    if sparse.isspmatrix(X):\n",
        "        X = X.toarray()\n",
        "        X = X.astype('float32')\n",
        "        pred_val=pred[idx_list_test[st_idx:end_idx]]\n",
        "        pred_val[X.nonzero()]=-np.inf\n",
        "        n100_list.append(NDCG(pred_val, test[st_idx:end_idx], k=100))\n",
        "        r20_list.append(Recall(pred_val, test[st_idx:end_idx], k=20))\n",
        "        r50_list.append(Recall(pred_val, test[st_idx:end_idx], k=50))\n",
        "n100_list = np.concatenate(n100_list)\n",
        "r20_list = np.concatenate(r20_list)\n",
        "r50_list = np.concatenate(r50_list)\n",
        "\n",
        "print(\"Test NDCG@100=%.5f (%.5f)\" % (np.mean(n100_list), np.std(n100_list) / np.sqrt(len(n100_list))))\n",
        "print(\"Test Recall@20=%.5f (%.5f)\" % (np.mean(r20_list), np.std(r20_list) / np.sqrt(len(r20_list))))\n",
        "print(\"Test Recall@50=%.5f (%.5f)\" % (np.mean(r50_list), np.std(r50_list) / np.sqrt(len(r50_list))))"
      ],
      "metadata": {
        "id": "YfJ0GtuFU6Jt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}