{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cars1015/recsys_MyReserch/blob/main/Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EpISYqIvcmYl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0a0f34e-2da0-4aaa-fc1a-dfd8552922cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHnZ2bRiUcCB"
      },
      "outputs": [],
      "source": [
        "import urllib#URlを扱うためのモジュール\n",
        "import urllib.request#URLを開いて読むためのモジュール\n",
        "import os#osに依存する機能を使用するためのもの\n",
        "import shutil#ファイル操作に関するモジュール\n",
        "import pandas as pd#データ解析を容易にするモジュール\n",
        "import numpy as np#数値計算を行うためのモジュール\n",
        "import sys#pythonの実行環境に関する情報を扱うためのライブラリ\n",
        "import zipfile#zipfileを扱うためのモジュール\n",
        "import distutils\n",
        "import distutils.util\n",
        "\n",
        "#zipファイルを持ってきてそのファイルを指定のパスに保存\n",
        "def save_zip_data(write_path, zip_url):\n",
        "    # zip_url = \"http://files.grouplens.org/datasets/movielens/ml-20m.zip\"\n",
        "    print('reading response')\n",
        "    with urllib.request.urlopen(zip_url) as response:\n",
        "      #ファイル内容の読み込み\n",
        "        zip_file = response.read()\n",
        "    print('writing zip file')\n",
        "    with open(write_path, 'wb') as f:\n",
        "        f.write(zip_file)\n",
        "#ml-20mのダウンロードとデータの移動\n",
        "def maybe_download_and_extract_movie_data(data_dir, force_overwrite=False):\n",
        "    write_path = os.path.join(data_dir, 'ml-20m.zip')\n",
        "    zip_url = \"http://files.grouplens.org/datasets/movielens/ml-20m.zip\"\n",
        "    if not os.path.isfile(write_path):#パスにファイルが存在しないときの処理\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"ml-20m\")#摘出先ディレクトリのパス設定\n",
        "    if os.path.isdir(extract_destination):#フォルダがある場合の処理\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)#ディレクトリの削除\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:#zipfileの読み込み\n",
        "        zip_ref.extractall(extract_destination)\n",
        "\n",
        "    current_dir = os.path.join(data_dir, 'ml-20m', 'ml-20m')\n",
        "    temp_dir = os.path.join(data_dir, 'ml-20m-temp')\n",
        "    right_dir = os.path.join(data_dir, 'ml-20m')\n",
        "\n",
        "    print(\"Moving stuff where it should be\")\n",
        "    #ここでファイルをすべてright_dirに移動させている。\n",
        "    shutil.move(current_dir, temp_dir)\n",
        "    shutil.rmtree(right_dir)\n",
        "    shutil.move(temp_dir, right_dir)\n",
        "\n",
        "    print('all extracted... in right place too')\n",
        "\n",
        "#netflix-prizeのダウンロードと展開\n",
        "def maybe_download_and_extract_netflix_data(data_dir, force_overwrite=False):\n",
        "    # NOTE: This doesn't work, because the URL is wrong. Stupid kaggle.\n",
        "    # NOTE This works now because I'm hosting the dataset. Hope that's not an expensive thing to do.\n",
        "\n",
        "    write_path = os.path.join(data_dir, 'netflix-prize.zip')#パスの結合\n",
        "    zip_url = \"https://s3-us-west-2.amazonaws.com/cf-datasets/netflix-prize-data.zip\"\n",
        "    if not os.path.isfile(write_path):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"netflix-prize\")\n",
        "    if os.path.isdir(extract_destination):\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:#データの展開\n",
        "        zip_ref.extractall(extract_destination)\n",
        "    print('all extracted...')\n",
        "#msd.zipデータの取得と展開\n",
        "def maybe_download_and_extract_msd(data_dir, force_overwrite=False):\n",
        "    write_path = os.path.join(data_dir, 'msd.zip')\n",
        "    zip_url = \"http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip\"\n",
        "    if not os.path.isfile(write_path):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"msd\")\n",
        "    if os.path.isdir(extract_destination):\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_destination)\n",
        "    print('all extracted...')\n",
        "#4つのcombined_data_{}.txtよりrating.csvにデータを書き込み\"movieId,userId,rating,timestamp\"の形\n",
        "def munge_netflix_data(data_dir, force_overwrite=False):\n",
        "    \"\"\"\n",
        "    Too tired to work. What I want to do is: iterate through. Whenever I find a line that\n",
        "    says \"4:\" or something, I know that's the movie-ID. Save it to movie_id variable.\n",
        "    Then, you can use that as the first part of a line for that user-item pair.\n",
        "    Also, I need to add a header. Once all that is good, I'll have a similar file\n",
        "    to ratings.csv for ml-20m.\n",
        "    \"\"\"\n",
        "    # NOTE: Is it going to be a problem that the user-ids have missing items? I don't know...\n",
        "    import re\n",
        "    # for file_number in [1,2,3,4]:\n",
        "    goal_file_path = os.path.join(data_dir, 'netflix-prize', 'ratings.csv')\n",
        "    if os.path.exists(goal_file_path):#rating.csvがある場合の処理\n",
        "        if not force_overwrite:\n",
        "            print(\"Looks like goal file already exists. Not overwriting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Something is there. Deleting it.\")\n",
        "            os.remove(goal_file_path)\n",
        "    #ここからrating.csvが存在しないとき\n",
        "    with open(goal_file_path, 'w') as ratings_file:\n",
        "        # headers will be in different order than ml-20m, but the way its parsed, it doesn't matter at all.\n",
        "        ratings_file.write(\"movieId,userId,rating,timestamp\\n\")#映画ID、ユーザID、レーティング、タイムスタンプの形で記入\n",
        "\n",
        "        for file_number in [1,2,3,4]:\n",
        "            print(\"Processing file {}\".format(file_number))\n",
        "            #ここでcombined_data_{}.txtの作成\n",
        "            read_file_path = os.path.join(data_dir, 'netflix-prize', 'combined_data_{}.txt'.format(file_number))\n",
        "            with open(read_file_path, 'r') as data_file:\n",
        "\n",
        "                movie_id = None\n",
        "                for line in data_file.readlines():#1行ずつ\n",
        "                    # print(line)\n",
        "                    # continue\n",
        "                    matches = re.match(r'^(\\d+):\\n$', line)#\\d+は連続する数字を意味、\n",
        "\n",
        "                    if matches:\n",
        "                      #matches[1]は映画IDを表す\n",
        "                        movie_id = matches[1] #the first is the whole match, the second is the part in parens.\n",
        "                    else:\n",
        "                        if movie_id is None:\n",
        "                            raise Exception(\"movie_id shouldn't be none\")#movieIDが登録されていないものがあったときの処理\n",
        "                        line_list = line.split(\",\")#コロンで要素分割\n",
        "                        assert len(line_list) == 3#ユーザID、評価、タイムスタンプが入っているかチェック\n",
        "                        # reorder so its the same as the other one...\n",
        "                        new_line = [line_list[0]]\n",
        "                        line = str(movie_id)+\",\"+line\n",
        "                        # print('writing line: {}.format(line))\n",
        "                        ratings_file.write(line)#rating.csvファイルに書き込み\n",
        "\n",
        "#train_triples.txtのデータをrating.csvに記入しているが中身分かってないおそらく映画ID、ユーザID、評価、timestampの形\n",
        "def munge_msd(data_dir, force_overwrite=False):\n",
        "    # It's really user, song, play-count. I need to re-order a bit. Also, timestamp gets ignored,\n",
        "    # so I'll just put a default. Also also, if it's in the file, it should be included.\n",
        "    goal_file_path = os.path.join(data_dir, 'msd', 'ratings.csv')\n",
        "    if os.path.exists(goal_file_path):\n",
        "        if not force_overwrite:\n",
        "            print(\"Looks like goal file already exists. Not overwriting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Something is there. Deleting it.\")\n",
        "            os.remove(goal_file_path)\n",
        "\n",
        "    with open(goal_file_path, 'w') as ratings_file:\n",
        "        # headers will be in different order than ml-20m, but the way its parsed, it doesn't matter at all.\n",
        "        ratings_file.write(\"movieId,userId,rating,timestamp\\n\")\n",
        "\n",
        "        read_file_path = os.path.join(data_dir, 'msd', 'train_triplets.txt')\n",
        "        with open(read_file_path, 'r') as data_file:\n",
        "            for line in data_file.readlines():\n",
        "              #空白文字を取り除きタブで分割\n",
        "                line_list = line.strip().split('\\t')\n",
        "                assert len(line_list) == 3\n",
        "                #タイムスタンプはN-Aとなる\n",
        "                new_line_list = [line_list[1], line_list[0], line_list[2], \"N-A\"]\n",
        "                new_line_string = \",\".join(new_line_list) + \"\\n\"\n",
        "                ratings_file.write(new_line_string)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#つかっていない？\n",
        "def load_train_data(csv_file):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    n_users = tp['uid'].max() + 1#列ごとの最大値+1が返される\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data\n",
        "\n",
        "#これも使っていない？\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "#データの整形\n",
        "def process_unzipped_data(DATA_DIR,\n",
        "                          force_overwrite=True,\n",
        "                          n_heldout_users=10000,\n",
        "                          discard_ratings_below=3.5,#この数字以下のレビューを切り捨てる\n",
        "                          min_users_per_item_to_include=0,\n",
        "                          min_clicks_per_user_to_include=5):\n",
        "\n",
        "    pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "    if os.path.isdir(pro_dir):\n",
        "        if force_overwrite:\n",
        "            print(\"Deleting processed-directory and recreating...\")\n",
        "            shutil.rmtree(pro_dir)\n",
        "        else:\n",
        "            print(\"pro_sg dir already exists. Exiting.\")\n",
        "            return\n",
        "    raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)\n",
        "\n",
        "\n",
        "    # In[4]:\n",
        "\n",
        "\n",
        "    # binarize the data (only keep ratings >= 4)\n",
        "    #レーティングが設定より低いもの削除\n",
        "    raw_data = raw_data[raw_data['rating'] > discard_ratings_below]\n",
        "\n",
        "\n",
        "    # ### Data splitting procedure\n",
        "\n",
        "    # - Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
        "    # - Use all the items from the training users as item set\n",
        "    # - For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction\n",
        "\n",
        "    # In[6]:\n",
        "\n",
        "\n",
        "#カウント数がこの関数で求められる\n",
        "    def get_count(tp, id):\n",
        "        playcount_groupbyid = tp[[id]].groupby(id, as_index=True)#idごとのグループ作成\n",
        "        count = playcount_groupbyid.size()#idのサンプル数がわかる。\n",
        "        return count\n",
        "\n",
        "\n",
        "    # In[7]:\n",
        "\n",
        "#おそらくmin_scはその映画の評価（クリック）された回数min_ucはユーザーのクリックした回数\n",
        "#ここでデータを限定し、限定したデータの標本数を返す\n",
        "    def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "        # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
        "        if min_sc > 0:\n",
        "            itemcount = get_count(tp, 'movieId')\n",
        "            tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]#min_sc以上のもののみに限定\n",
        "\n",
        "        # Only keep the triplets for users who clicked on at least min_uc items\n",
        "        # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
        "        if min_uc > 0:\n",
        "            usercount = get_count(tp, 'userId')\n",
        "            tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]#min_uc以上のものに限定\n",
        "\n",
        "        # Update both usercount and itemcount after filtering\n",
        "        usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
        "        return tp, usercount, itemcount\n",
        "\n",
        "\n",
        "    # Only keep items that are clicked on by at least 5 users\n",
        "\n",
        "    # In[8]:\n",
        "\n",
        "   #多分￥部分に下の関数の返り値が来ている\n",
        "    raw_data, user_activity, item_popularity = \\\n",
        "        filter_triplets(raw_data,\n",
        "                        min_uc=min_clicks_per_user_to_include,\n",
        "                        min_sc=min_users_per_item_to_include)\n",
        "\n",
        "\n",
        "    # In[9]:\n",
        "    #スパース性の計算shape[0]は行列の行の数しめす\n",
        "    sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
        "\n",
        "    print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" %\n",
        "          (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))\n",
        "\n",
        "\n",
        "    # In[10]:\n",
        "\n",
        "\n",
        "    unique_uid = user_activity.index\n",
        "  #シード値の設定\n",
        "    np.random.seed(98765)\n",
        "    #unique_uid.sizeの大きさのランダムな配列を作成\n",
        "    idx_perm = np.random.permutation(unique_uid.size)\n",
        "    #これはユーザーIDをランダムに並べ替えたもの\n",
        "    unique_uid = unique_uid[idx_perm]\n",
        "\n",
        "\n",
        "    # In[11]:\n",
        "\n",
        "\n",
        "    # create train/validation/test users\n",
        "    n_users = unique_uid.size\n",
        "    # n_heldout_users = 10000に設定中これは検証テストのために準備されたユーザ数\n",
        "    tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "    vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "    te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "\n",
        "    # In[12]:\n",
        "\n",
        "#row_dataからtrainigデータに含まれるものを抽出\n",
        "    train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
        "\n",
        "\n",
        "    # In[13]:\n",
        "\n",
        "    #トレイニングデータに含まれる映画のIDを抽出\n",
        "    unique_sid = pd.unique(train_plays['movieId'])\n",
        "\n",
        "    # In[14]:\n",
        "\n",
        "    #enumerateでインデックスとデータの形でfor文回せる\n",
        "    #この2文で映画IDとユーザIDの辞書が出来る。\n",
        "    show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "    profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "\n",
        "    # In[15]:\n",
        "\n",
        "\n",
        "    #映画IDとユーザID一覧をtxtファイルに書き込みする。\n",
        "    if not os.path.exists(pro_dir):\n",
        "        os.makedirs(pro_dir)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "        for sid in unique_sid:\n",
        "            f.write('%s\\n' % sid)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
        "        for uid in unique_uid:\n",
        "            f.write('%s\\n' % uid)\n",
        "\n",
        "    # In[16]:\n",
        "\n",
        "\n",
        "    def split_train_test_proportion(data, test_prop=0.2):\n",
        "        data_grouped_by_user = data.groupby('userId')\n",
        "        tr_list, te_list = list(), list()\n",
        "\n",
        "        np.random.seed(98765)\n",
        "\n",
        "        for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "          #ユーザーの評価したデータ\n",
        "            n_items_u = len(group)\n",
        "            #ユーザが5回以上評価したデータに関してはランダムでテストデータにも入れる。(0.2の割合)\n",
        "            if n_items_u >= 5:\n",
        "              #中身がFalseで大きさがn_items_uの配列をidxに格納\n",
        "                idx = np.zeros(n_items_u, dtype='bool')\n",
        "                #replace=Falseで値の重複なし、配列のサイズは0.2*アイテム数、\n",
        "                idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "                tr_list.append(group[np.logical_not(idx)])\n",
        "                te_list.append(group[idx])\n",
        "            else:\n",
        "                tr_list.append(group)\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(\"%d users sampled\" % i)\n",
        "                sys.stdout.flush()\n",
        "#リストを結合\n",
        "        data_tr = pd.concat(tr_list)\n",
        "        data_te = pd.concat(te_list)\n",
        "\n",
        "        return data_tr, data_te\n",
        "\n",
        "\n",
        "    # In[17]:\n",
        "\n",
        "    #検証データ内に含まれるデータを抽出\n",
        "    vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "    #検証データ内に含まれる映画のIDを取得\n",
        "    vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "\n",
        "    # In[18]:\n",
        "\n",
        "    #トレイニングデータ、テスト用データに分ける\n",
        "    vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "\n",
        "    # In[19]:\n",
        "    #同様の処理をテストデータにも行う\n",
        "    test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "    test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "    # In[20]:\n",
        "\n",
        "\n",
        "    test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "\n",
        "    # ### Save the data into (user_index, item_index) format\n",
        "\n",
        "    # In[21]:\n",
        "\n",
        "\n",
        "    def numerize(tp):\n",
        "        uid = map(lambda x: profile2id[x], tp['userId'])\n",
        "        sid = map(lambda x: show2id[x], tp['movieId'])\n",
        "        return pd.DataFrame(data={'uid': list(uid), 'sid': list(sid)}, columns=['uid', 'sid'])\n",
        "\n",
        "\n",
        "    # In[22]:\n",
        "\n",
        "\n",
        "    train_data = numerize(train_plays)\n",
        "    #インデックスなしでcsvファイルに書き出し\n",
        "    train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "\n",
        "    # In[23]:\n",
        "\n",
        "\n",
        "    vad_data_tr = numerize(vad_plays_tr)\n",
        "    vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "\n",
        "    # In[24]:\n",
        "\n",
        "\n",
        "    vad_data_te = numerize(vad_plays_te)\n",
        "    vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "\n",
        "    # In[25]:\n",
        "\n",
        "\n",
        "    test_data_tr = numerize(test_plays_tr)\n",
        "    test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "\n",
        "    # In[26]:\n",
        "\n",
        "\n",
        "    test_data_te = numerize(test_plays_te)\n",
        "    test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\"--use-noise-morpher\", help=\"Whether to use noise-morphing or not. Defaults to True.\", type=lambda x:bool(distutils.util.strtobool(x)), default=defaults[\"use_noise_morpher\"])\n",
        "    parser.add_argument(\"--force-overwrite\", help=\"Re-download, extract, and parse data\", type=lambda x:bool(distutils.util.strtobool(x)), default=False)\n",
        "    parser.add_argument(\"--dataset\", help=\"Which dataset do you want?\", type=str, default='ml-20m')\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    force_overwrite = args.force_overwrite\n",
        "    dataset = 'all' # args.dataset\n",
        "    assert dataset in ['ml-20m', 'netflix-prize', 'msd', 'all']\n",
        "\n",
        "    if dataset == 'ml-20m' or dataset == 'all':\n",
        "        print(\"Doing ml-20m stuff!\")\n",
        "        maybe_download_and_extract_movie_data(\"/content/drive/MyDrive/recommend\", force_overwrite=force_overwrite)\n",
        "        process_unzipped_data('/content/drive/MyDrive/recommend/ml-20m', force_overwrite=force_overwrite)\n",
        "\n",
        "    if dataset == 'netflix-prize' or dataset == 'all':\n",
        "        print(\"Doing netflix-prize stuff!\")\n",
        "        maybe_download_and_extract_netflix_data('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        munge_netflix_data('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        process_unzipped_data('/content/drive/MyDrive/recommend/netflix-prize', force_overwrite=force_overwrite, n_heldout_users=40000)\n",
        "\n",
        "    if dataset == 'msd' or dataset == 'all':\n",
        "        maybe_download_and_extract_msd('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        munge_msd('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        process_unzipped_data(\n",
        "            '/content/drive/MyDrive/recommend/msd',\n",
        "            force_overwrite=True,\n",
        "            n_heldout_users=50000,\n",
        "            discard_ratings_below=0.0,\n",
        "            min_users_per_item_to_include=200,\n",
        "            min_clicks_per_user_to_include=20)\n",
        "    print(\"All done!\")\n",
        "    exit()"
      ]
    }
  ]
}