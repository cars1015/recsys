{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cars1015/recsys_MyReserch/blob/main/Preprocess.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Googleドライブをマウント\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EpISYqIvcmYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHnZ2bRiUcCB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c627061d-7fe3-401d-9b13-37f2a50c052f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Doing ml-20m stuff!\n",
            "Zip not downloaded. Downloading now...\n",
            "reading response\n",
            "writing zip file\n",
            "Zip downloaded\n",
            "unzipping data\n",
            "Moving stuff where it should be\n",
            "all extracted... in right place too\n",
            "After filtering, there are 9990682 watching events from 136677 users and 20720 movies (sparsity: 0.353%)\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "Doing netflix-prize stuff!\n",
            "Zip not downloaded. Downloading now...\n",
            "reading response\n",
            "writing zip file\n",
            "Zip downloaded\n",
            "unzipping data\n",
            "all extracted...\n",
            "Processing file 1\n",
            "Processing file 2\n",
            "Processing file 3\n",
            "Processing file 4\n",
            "After filtering, there are 56880037 watching events from 463435 users and 17769 movies (sparsity: 0.691%)\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "10000 users sampled\n",
            "11000 users sampled\n",
            "12000 users sampled\n",
            "13000 users sampled\n",
            "14000 users sampled\n",
            "15000 users sampled\n",
            "16000 users sampled\n",
            "17000 users sampled\n",
            "18000 users sampled\n",
            "19000 users sampled\n",
            "20000 users sampled\n",
            "21000 users sampled\n",
            "22000 users sampled\n",
            "23000 users sampled\n",
            "24000 users sampled\n",
            "25000 users sampled\n",
            "26000 users sampled\n",
            "27000 users sampled\n",
            "28000 users sampled\n",
            "29000 users sampled\n",
            "30000 users sampled\n",
            "31000 users sampled\n",
            "32000 users sampled\n",
            "33000 users sampled\n",
            "34000 users sampled\n",
            "35000 users sampled\n",
            "36000 users sampled\n",
            "37000 users sampled\n",
            "38000 users sampled\n",
            "39000 users sampled\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "10000 users sampled\n",
            "11000 users sampled\n",
            "12000 users sampled\n",
            "13000 users sampled\n",
            "14000 users sampled\n",
            "15000 users sampled\n",
            "16000 users sampled\n",
            "17000 users sampled\n",
            "18000 users sampled\n",
            "19000 users sampled\n",
            "20000 users sampled\n",
            "21000 users sampled\n",
            "22000 users sampled\n",
            "23000 users sampled\n",
            "24000 users sampled\n",
            "25000 users sampled\n",
            "26000 users sampled\n",
            "27000 users sampled\n",
            "28000 users sampled\n",
            "29000 users sampled\n",
            "30000 users sampled\n",
            "31000 users sampled\n",
            "32000 users sampled\n",
            "33000 users sampled\n",
            "34000 users sampled\n",
            "35000 users sampled\n",
            "36000 users sampled\n",
            "37000 users sampled\n",
            "38000 users sampled\n",
            "39000 users sampled\n",
            "Zip not downloaded. Downloading now...\n",
            "reading response\n",
            "writing zip file\n",
            "Zip downloaded\n",
            "unzipping data\n",
            "all extracted...\n",
            "After filtering, there are 33633450 watching events from 571355 users and 41140 movies (sparsity: 0.143%)\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "10000 users sampled\n",
            "11000 users sampled\n",
            "12000 users sampled\n",
            "13000 users sampled\n",
            "14000 users sampled\n",
            "15000 users sampled\n",
            "16000 users sampled\n",
            "17000 users sampled\n",
            "18000 users sampled\n",
            "19000 users sampled\n",
            "20000 users sampled\n",
            "21000 users sampled\n",
            "22000 users sampled\n",
            "23000 users sampled\n",
            "24000 users sampled\n",
            "25000 users sampled\n",
            "26000 users sampled\n",
            "27000 users sampled\n",
            "28000 users sampled\n",
            "29000 users sampled\n",
            "30000 users sampled\n",
            "31000 users sampled\n",
            "32000 users sampled\n",
            "33000 users sampled\n",
            "34000 users sampled\n",
            "35000 users sampled\n",
            "36000 users sampled\n",
            "37000 users sampled\n",
            "38000 users sampled\n",
            "39000 users sampled\n",
            "40000 users sampled\n",
            "41000 users sampled\n",
            "42000 users sampled\n",
            "43000 users sampled\n",
            "44000 users sampled\n",
            "45000 users sampled\n",
            "46000 users sampled\n",
            "47000 users sampled\n",
            "48000 users sampled\n",
            "49000 users sampled\n",
            "0 users sampled\n",
            "1000 users sampled\n",
            "2000 users sampled\n",
            "3000 users sampled\n",
            "4000 users sampled\n",
            "5000 users sampled\n",
            "6000 users sampled\n",
            "7000 users sampled\n",
            "8000 users sampled\n",
            "9000 users sampled\n",
            "10000 users sampled\n",
            "11000 users sampled\n",
            "12000 users sampled\n",
            "13000 users sampled\n",
            "14000 users sampled\n",
            "15000 users sampled\n",
            "16000 users sampled\n",
            "17000 users sampled\n",
            "18000 users sampled\n",
            "19000 users sampled\n",
            "20000 users sampled\n",
            "21000 users sampled\n",
            "22000 users sampled\n",
            "23000 users sampled\n",
            "24000 users sampled\n",
            "25000 users sampled\n",
            "26000 users sampled\n",
            "27000 users sampled\n",
            "28000 users sampled\n",
            "29000 users sampled\n",
            "30000 users sampled\n",
            "31000 users sampled\n",
            "32000 users sampled\n",
            "33000 users sampled\n",
            "34000 users sampled\n",
            "35000 users sampled\n",
            "36000 users sampled\n",
            "37000 users sampled\n",
            "38000 users sampled\n",
            "39000 users sampled\n",
            "40000 users sampled\n",
            "41000 users sampled\n",
            "42000 users sampled\n",
            "43000 users sampled\n",
            "44000 users sampled\n",
            "45000 users sampled\n",
            "46000 users sampled\n",
            "47000 users sampled\n",
            "48000 users sampled\n",
            "49000 users sampled\n",
            "All done!\n"
          ]
        }
      ],
      "source": [
        "import urllib\n",
        "import urllib.request\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import zipfile\n",
        "import distutils\n",
        "import distutils.util\n",
        "\n",
        "\n",
        "def save_zip_data(write_path, zip_url):\n",
        "    # zip_url = \"http://files.grouplens.org/datasets/movielens/ml-20m.zip\"\n",
        "    print('reading response')\n",
        "    with urllib.request.urlopen(zip_url) as response:\n",
        "        zip_file = response.read()\n",
        "    print('writing zip file')\n",
        "    with open(write_path, 'wb') as f:\n",
        "        f.write(zip_file)\n",
        "\n",
        "def maybe_download_and_extract_movie_data(data_dir, force_overwrite=False):\n",
        "    write_path = os.path.join(data_dir, 'ml-20m.zip')\n",
        "    zip_url = \"http://files.grouplens.org/datasets/movielens/ml-20m.zip\"\n",
        "    if not os.path.isfile(write_path):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"ml-20m\")\n",
        "    if os.path.isdir(extract_destination):\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_destination)\n",
        "\n",
        "    current_dir = os.path.join(data_dir, 'ml-20m', 'ml-20m')\n",
        "    temp_dir = os.path.join(data_dir, 'ml-20m-temp')\n",
        "    right_dir = os.path.join(data_dir, 'ml-20m')\n",
        "\n",
        "    print(\"Moving stuff where it should be\")\n",
        "    shutil.move(current_dir, temp_dir)\n",
        "    shutil.rmtree(right_dir)\n",
        "    shutil.move(temp_dir, right_dir)\n",
        "\n",
        "    print('all extracted... in right place too')\n",
        "\n",
        "\n",
        "def maybe_download_and_extract_netflix_data(data_dir, force_overwrite=False):\n",
        "    # NOTE: This doesn't work, because the URL is wrong. Stupid kaggle.\n",
        "    # NOTE This works now because I'm hosting the dataset. Hope that's not an expensive thing to do.\n",
        "\n",
        "    write_path = os.path.join(data_dir, 'netflix-prize.zip')\n",
        "    zip_url = \"https://s3-us-west-2.amazonaws.com/cf-datasets/netflix-prize-data.zip\"\n",
        "    if not os.path.isfile(write_path):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"netflix-prize\")\n",
        "    if os.path.isdir(extract_destination):\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_destination)\n",
        "    print('all extracted...')\n",
        "\n",
        "def maybe_download_and_extract_msd(data_dir, force_overwrite=False):\n",
        "    write_path = os.path.join(data_dir, 'msd.zip')\n",
        "    zip_url = \"http://labrosa.ee.columbia.edu/millionsong/sites/default/files/challenge/train_triplets.txt.zip\"\n",
        "    if not os.path.isfile(write_path):\n",
        "        os.makedirs(data_dir, exist_ok=True)\n",
        "        print(\"Zip not downloaded. Downloading now...\")\n",
        "        save_zip_data(write_path, zip_url)\n",
        "        print(\"Zip downloaded\")\n",
        "    else:\n",
        "        print(\"Zip already downloaded\")\n",
        "\n",
        "    extract_destination = os.path.join(data_dir, \"msd\")\n",
        "    if os.path.isdir(extract_destination):\n",
        "        if not force_overwrite:\n",
        "            print(\"seems extracted datadir already exists, and not forcing overwrite. Exiting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Deleting extracted-lib and recreating...\")\n",
        "            shutil.rmtree(extract_destination)\n",
        "    print('unzipping data')\n",
        "    with zipfile.ZipFile(write_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_destination)\n",
        "    print('all extracted...')\n",
        "\n",
        "def munge_netflix_data(data_dir, force_overwrite=False):\n",
        "    \"\"\"\n",
        "    Too tired to work. What I want to do is: iterate through. Whenever I find a line that\n",
        "    says \"4:\" or something, I know that's the movie-ID. Save it to movie_id variable.\n",
        "    Then, you can use that as the first part of a line for that user-item pair.\n",
        "    Also, I need to add a header. Once all that is good, I'll have a similar file\n",
        "    to ratings.csv for ml-20m.\n",
        "    \"\"\"\n",
        "    # NOTE: Is it going to be a problem that the user-ids have missing items? I don't know...\n",
        "    import re\n",
        "    # for file_number in [1,2,3,4]:\n",
        "    goal_file_path = os.path.join(data_dir, 'netflix-prize', 'ratings.csv')\n",
        "    if os.path.exists(goal_file_path):\n",
        "        if not force_overwrite:\n",
        "            print(\"Looks like goal file already exists. Not overwriting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Something is there. Deleting it.\")\n",
        "            os.remove(goal_file_path)\n",
        "\n",
        "    with open(goal_file_path, 'w') as ratings_file:\n",
        "        # headers will be in different order than ml-20m, but the way its parsed, it doesn't matter at all.\n",
        "        ratings_file.write(\"movieId,userId,rating,timestamp\\n\")\n",
        "\n",
        "        for file_number in [1,2,3,4]:\n",
        "            print(\"Processing file {}\".format(file_number))\n",
        "            read_file_path = os.path.join(data_dir, 'netflix-prize', 'combined_data_{}.txt'.format(file_number))\n",
        "            with open(read_file_path, 'r') as data_file:\n",
        "\n",
        "                movie_id = None\n",
        "                for line in data_file.readlines():\n",
        "                    # print(line)\n",
        "                    # continue\n",
        "                    matches = re.match(r'^(\\d+):\\n$', line)\n",
        "\n",
        "                    if matches:\n",
        "                        movie_id = matches[1] #the first is the whole match, the second is the part in parens.\n",
        "                    else:\n",
        "                        if movie_id is None:\n",
        "                            raise Exception(\"movie_id shouldn't be none\")\n",
        "                        line_list = line.split(\",\")\n",
        "                        assert len(line_list) == 3\n",
        "                        # reorder so its the same as the other one...\n",
        "                        new_line = [line_list[0]]\n",
        "                        line = str(movie_id)+\",\"+line\n",
        "                        # print('writing line: {}.format(line))\n",
        "                        ratings_file.write(line)\n",
        "\n",
        "def munge_msd(data_dir, force_overwrite=False):\n",
        "    # It's really user, song, play-count. I need to re-order a bit. Also, timestamp gets ignored,\n",
        "    # so I'll just put a default. Also also, if it's in the file, it should be included.\n",
        "    goal_file_path = os.path.join(data_dir, 'msd', 'ratings.csv')\n",
        "    if os.path.exists(goal_file_path):\n",
        "        if not force_overwrite:\n",
        "            print(\"Looks like goal file already exists. Not overwriting.\")\n",
        "            return\n",
        "        else:\n",
        "            print(\"Something is there. Deleting it.\")\n",
        "            os.remove(goal_file_path)\n",
        "\n",
        "    with open(goal_file_path, 'w') as ratings_file:\n",
        "        # headers will be in different order than ml-20m, but the way its parsed, it doesn't matter at all.\n",
        "        ratings_file.write(\"movieId,userId,rating,timestamp\\n\")\n",
        "\n",
        "        read_file_path = os.path.join(data_dir, 'msd', 'train_triplets.txt')\n",
        "        with open(read_file_path, 'r') as data_file:\n",
        "            for line in data_file.readlines():\n",
        "                line_list = line.strip().split('\\t')\n",
        "                assert len(line_list) == 3\n",
        "                new_line_list = [line_list[1], line_list[0], line_list[2], \"N-A\"]\n",
        "                new_line_string = \",\".join(new_line_list) + \"\\n\"\n",
        "                ratings_file.write(new_line_string)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def load_train_data(csv_file):\n",
        "    tp = pd.read_csv(csv_file)\n",
        "    n_users = tp['uid'].max() + 1\n",
        "\n",
        "    rows, cols = tp['uid'], tp['sid']\n",
        "    data = sparse.csr_matrix((np.ones_like(rows),\n",
        "                             (rows, cols)), dtype='float64',\n",
        "                             shape=(n_users, n_items))\n",
        "    return data\n",
        "\n",
        "\n",
        "def load_tr_te_data(csv_file_tr, csv_file_te):\n",
        "    tp_tr = pd.read_csv(csv_file_tr)\n",
        "    tp_te = pd.read_csv(csv_file_te)\n",
        "\n",
        "    start_idx = min(tp_tr['uid'].min(), tp_te['uid'].min())\n",
        "    end_idx = max(tp_tr['uid'].max(), tp_te['uid'].max())\n",
        "\n",
        "    rows_tr, cols_tr = tp_tr['uid'] - start_idx, tp_tr['sid']\n",
        "    rows_te, cols_te = tp_te['uid'] - start_idx, tp_te['sid']\n",
        "\n",
        "    data_tr = sparse.csr_matrix((np.ones_like(rows_tr),\n",
        "                             (rows_tr, cols_tr)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    data_te = sparse.csr_matrix((np.ones_like(rows_te),\n",
        "                             (rows_te, cols_te)), dtype='float64', shape=(end_idx - start_idx + 1, n_items))\n",
        "    return data_tr, data_te\n",
        "\n",
        "\n",
        "def process_unzipped_data(DATA_DIR,\n",
        "                          force_overwrite=False,\n",
        "                          n_heldout_users=10000,\n",
        "                          discard_ratings_below=3.5,\n",
        "                          min_users_per_item_to_include=0,\n",
        "                          min_clicks_per_user_to_include=5):\n",
        "\n",
        "    pro_dir = os.path.join(DATA_DIR, 'pro_sg')\n",
        "    if os.path.isdir(pro_dir):\n",
        "        if force_overwrite:\n",
        "            print(\"Deleting processed-directory and recreating...\")\n",
        "            shutil.rmtree(pro_dir)\n",
        "        else:\n",
        "            print(\"pro_sg dir already exists. Exiting.\")\n",
        "            return\n",
        "    raw_data = pd.read_csv(os.path.join(DATA_DIR, 'ratings.csv'), header=0)\n",
        "\n",
        "\n",
        "\n",
        "    # binarize the data (only keep ratings >= 4)\n",
        "    raw_data = raw_data[raw_data['rating'] > discard_ratings_below]\n",
        "\n",
        "\n",
        "    # ### Data splitting procedure\n",
        "\n",
        "    # - Select 10K users as heldout users, 10K users as validation users, and the rest of the users for training\n",
        "    # - Use all the items from the training users as item set\n",
        "    # - For each of both validation and test user, subsample 80% as fold-in data and the rest for prediction\n",
        "\n",
        "\n",
        "    def get_count(tp, id):\n",
        "        playcount_groupbyid = tp[[id]].groupby(id, as_index=True)\n",
        "        count = playcount_groupbyid.size()\n",
        "        return count\n",
        "\n",
        "\n",
        "\n",
        "    def filter_triplets(tp, min_uc=5, min_sc=0):\n",
        "        # Only keep the triplets for items which were clicked on by at least min_sc users.\n",
        "        if min_sc > 0:\n",
        "            itemcount = get_count(tp, 'movieId')\n",
        "            tp = tp[tp['movieId'].isin(itemcount.index[itemcount >= min_sc])]\n",
        "\n",
        "        # Only keep the triplets for users who clicked on at least min_uc items\n",
        "        # After doing this, some of the items will have less than min_uc users, but should only be a small proportion\n",
        "        if min_uc > 0:\n",
        "            usercount = get_count(tp, 'userId')\n",
        "            tp = tp[tp['userId'].isin(usercount.index[usercount >= min_uc])]\n",
        "\n",
        "        # Update both usercount and itemcount after filtering\n",
        "        usercount, itemcount = get_count(tp, 'userId'), get_count(tp, 'movieId')\n",
        "        return tp, usercount, itemcount\n",
        "\n",
        "\n",
        "    # Only keep items that are clicked on by at least 5 users\n",
        "\n",
        "   \n",
        "    raw_data, user_activity, item_popularity = \\\n",
        "        filter_triplets(raw_data,\n",
        "                        min_uc=min_clicks_per_user_to_include,\n",
        "                        min_sc=min_users_per_item_to_include)\n",
        "    \n",
        "    sparsity = 1. * raw_data.shape[0] / (user_activity.shape[0] * item_popularity.shape[0])\n",
        "\n",
        "    print(\"After filtering, there are %d watching events from %d users and %d movies (sparsity: %.3f%%)\" %\n",
        "          (raw_data.shape[0], user_activity.shape[0], item_popularity.shape[0], sparsity * 100))\n",
        "\n",
        "\n",
        "\n",
        "    unique_uid = user_activity.index\n",
        "\n",
        "    np.random.seed(98765)\n",
        "    idx_perm = np.random.permutation(unique_uid.size)\n",
        "    unique_uid = unique_uid[idx_perm]\n",
        "\n",
        "\n",
        "\n",
        "    # create train/validation/test users\n",
        "    n_users = unique_uid.size\n",
        "    # n_heldout_users = 10000\n",
        "\n",
        "    tr_users = unique_uid[:(n_users - n_heldout_users * 2)]\n",
        "    vd_users = unique_uid[(n_users - n_heldout_users * 2): (n_users - n_heldout_users)]\n",
        "    te_users = unique_uid[(n_users - n_heldout_users):]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    train_plays = raw_data.loc[raw_data['userId'].isin(tr_users)]\n",
        "\n",
        "\n",
        "\n",
        "    unique_sid = pd.unique(train_plays['movieId'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    show2id = dict((sid, i) for (i, sid) in enumerate(unique_sid))\n",
        "    profile2id = dict((pid, i) for (i, pid) in enumerate(unique_uid))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    if not os.path.exists(pro_dir):\n",
        "        os.makedirs(pro_dir)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_sid.txt'), 'w') as f:\n",
        "        for sid in unique_sid:\n",
        "            f.write('%s\\n' % sid)\n",
        "\n",
        "    with open(os.path.join(pro_dir, 'unique_uid.txt'), 'w') as f:\n",
        "        for uid in unique_uid:\n",
        "            f.write('%s\\n' % uid)\n",
        "\n",
        "\n",
        "    def split_train_test_proportion(data, test_prop=0.2):\n",
        "        data_grouped_by_user = data.groupby('userId')\n",
        "        tr_list, te_list = list(), list()\n",
        "\n",
        "        np.random.seed(98765)\n",
        "\n",
        "        for i, (_, group) in enumerate(data_grouped_by_user):\n",
        "            n_items_u = len(group)\n",
        "\n",
        "            if n_items_u >= 5:\n",
        "                idx = np.zeros(n_items_u, dtype='bool')\n",
        "                idx[np.random.choice(n_items_u, size=int(test_prop * n_items_u), replace=False).astype('int64')] = True\n",
        "\n",
        "                tr_list.append(group[np.logical_not(idx)])\n",
        "                te_list.append(group[idx])\n",
        "            else:\n",
        "                tr_list.append(group)\n",
        "\n",
        "            if i % 1000 == 0:\n",
        "                print(\"%d users sampled\" % i)\n",
        "                sys.stdout.flush()\n",
        "\n",
        "        data_tr = pd.concat(tr_list)\n",
        "        data_te = pd.concat(te_list)\n",
        "\n",
        "        return data_tr, data_te\n",
        "\n",
        "\n",
        "\n",
        "    vad_plays = raw_data.loc[raw_data['userId'].isin(vd_users)]\n",
        "    vad_plays = vad_plays.loc[vad_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "\n",
        "\n",
        "    vad_plays_tr, vad_plays_te = split_train_test_proportion(vad_plays)\n",
        "\n",
        "\n",
        "\n",
        "    test_plays = raw_data.loc[raw_data['userId'].isin(te_users)]\n",
        "    test_plays = test_plays.loc[test_plays['movieId'].isin(unique_sid)]\n",
        "\n",
        "\n",
        "\n",
        "    test_plays_tr, test_plays_te = split_train_test_proportion(test_plays)\n",
        "\n",
        "\n",
        "    # ### Save the data into (user_index, item_index) format\n",
        "\n",
        "\n",
        "    def numerize(tp):\n",
        "        uid = map(lambda x: profile2id[x], tp['userId'])\n",
        "        sid = map(lambda x: show2id[x], tp['movieId'])\n",
        "        return pd.DataFrame(data={'uid': list(uid), 'sid': list(sid)}, columns=['uid', 'sid'])\n",
        "\n",
        "\n",
        "\n",
        "    train_data = numerize(train_plays)\n",
        "    train_data.to_csv(os.path.join(pro_dir, 'train.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "    vad_data_tr = numerize(vad_plays_tr)\n",
        "    vad_data_tr.to_csv(os.path.join(pro_dir, 'validation_tr.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "    vad_data_te = numerize(vad_plays_te)\n",
        "    vad_data_te.to_csv(os.path.join(pro_dir, 'validation_te.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "    test_data_tr = numerize(test_plays_tr)\n",
        "    test_data_tr.to_csv(os.path.join(pro_dir, 'test_tr.csv'), index=False)\n",
        "\n",
        "\n",
        "\n",
        "    test_data_te = numerize(test_plays_te)\n",
        "    test_data_te.to_csv(os.path.join(pro_dir, 'test_te.csv'), index=False)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    # parser.add_argument(\"--use-noise-morpher\", help=\"Whether to use noise-morphing or not. Defaults to True.\", type=lambda x:bool(distutils.util.strtobool(x)), default=defaults[\"use_noise_morpher\"])\n",
        "    parser.add_argument(\"--force-overwrite\", help=\"Re-download, extract, and parse data\", type=lambda x:bool(distutils.util.strtobool(x)), default=False)\n",
        "    parser.add_argument(\"--dataset\", help=\"Which dataset do you want?\", type=str, default='ml-20m')\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_args(args=[])\n",
        "\n",
        "    force_overwrite = args.force_overwrite\n",
        "    dataset = 'all' # args.dataset\n",
        "    assert dataset in ['ml-20m', 'netflix-prize', 'msd', 'all']\n",
        "\n",
        "    if dataset == 'ml-20m' or dataset == 'all':\n",
        "        print(\"Doing ml-20m stuff!\")\n",
        "        maybe_download_and_extract_movie_data(\"/content/drive/MyDrive/recommend\", force_overwrite=force_overwrite)\n",
        "        process_unzipped_data('/content/drive/MyDrive/recommend/ml-20m', force_overwrite=force_overwrite)\n",
        "\n",
        "    if dataset == 'netflix-prize' or dataset == 'all':\n",
        "        print(\"Doing netflix-prize stuff!\")\n",
        "        maybe_download_and_extract_netflix_data('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        munge_netflix_data('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        process_unzipped_data('/content/drive/MyDrive/recommend/netflix-prize', force_overwrite=force_overwrite, n_heldout_users=40000)\n",
        "\n",
        "    if dataset == 'msd' or dataset == 'all':\n",
        "        maybe_download_and_extract_msd('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        munge_msd('/content/drive/MyDrive/recommend', force_overwrite=force_overwrite)\n",
        "        process_unzipped_data(\n",
        "            '/content/drive/MyDrive/recommend/msd',\n",
        "            force_overwrite=force_overwrite,\n",
        "            n_heldout_users=50000,\n",
        "            discard_ratings_below=0.0,\n",
        "            min_users_per_item_to_include=200,\n",
        "            min_clicks_per_user_to_include=20)\n",
        "\n",
        "    print(\"All done!\")\n",
        "    exit()"
      ]
    }
  ]
}